
"""
Streamlit web application for interacting with PDF documents.

This application allows users to upload a PDF file, ask questions about its content,
and receive answers generated by a Hugging Face language model.

The process involves:
1.  Loading environment variables for API keys and model configurations.
2.  Setting up the Streamlit user interface.
3.  Handling PDF file uploads.
4.  Extracting text from the uploaded PDF.
5.  Splitting the extracted text into manageable chunks.
6.  Generating embeddings for the text chunks using a Hugging Face model.
7.  Storing the embeddings in a Chroma vector store for efficient retrieval.
8.  Retrieving relevant text chunks based on user queries.
9.  Generating answers using a Hugging Face language model based on the retrieved context.
10. Displaying the answer and the retrieved context to the user.
11. Cleaning up the created vector store on application exit.
"""

import os
import shutil
import time
import streamlit as st
import pdfplumber
import tempfile
from dotenv import load_dotenv
import hashlib
import atexit
import gc

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from huggingface_hub import InferenceClient

# -----------------------------
# üîß Load environment variables
# -----------------------------
load_dotenv()

# Load Hugging Face API token and model configurations from environment variables
HF_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN")
DEFAULT_MODEL = os.getenv("DEFAULT_MODEL", "HuggingFaceH4/zephyr-7b-beta")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "sentence-transformers/all-MiniLM-L6-v2")
CHROMA_DB_DIR = os.getenv("CHROMA_DB_DIR", "chroma_langchain_hf")

# Set Hugging Face API token if available
if HF_TOKEN:
    os.environ["HUGGINGFACEHUB_API_TOKEN"] = HF_TOKEN

# -----------------------------
# ‚öôÔ∏è Streamlit setup
# -----------------------------
st.set_page_config(page_title="üìÑ Explore Your PDF", page_icon="üìö")
st.title("üìÑ Explore Your PDF ‚Äî Hugging Face Edition")
st.caption("Upload a PDF and ask questions about its content using a Hugging Face model.")

# -----------------------------
# ü§ó Model selection (sidebar)
# -----------------------------
st.sidebar.header("‚öôÔ∏è Model Settings")
model_name = st.sidebar.text_input("Model name (repo ID)", value=DEFAULT_MODEL)

# Check for Hugging Face token and display a warning if not found
if not HF_TOKEN:
    st.warning("No Hugging Face token found. Add it to your .env file.")
    st.stop()

# -----------------------------
# üì• Upload PDF
# -----------------------------
uploaded_file = st.file_uploader("üìé Upload a PDF file", type=["pdf"])

if uploaded_file:
    # Read the content of the uploaded PDF
    file_content = uploaded_file.read()
    # Generate a unique hash for the file content to use as a directory name
    file_hash = hashlib.md5(file_content).hexdigest()
    unique_chroma_db_dir = os.path.join(CHROMA_DB_DIR, file_hash)

    # Check if the uploaded file is new or different from the previous one
    if "last_uploaded_file_hash" not in st.session_state or st.session_state.last_uploaded_file_hash != file_hash:
        st.session_state.last_uploaded_file_hash = file_hash
        st.session_state.vectordb = None  # Clear previous vectordb

        # Save the uploaded file to a temporary location
        with tempfile.NamedTemporaryFile(delete=False) as tmp:
            tmp.write(file_content)
            pdf_path = tmp.name

        st.success("‚úÖ PDF uploaded successfully!")

        # -----------------------------
        # üìÑ Extract text
        # -----------------------------
        all_text = ""
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                text = page.extract_text()
                if text:
                    all_text += text + "\n"

        if not all_text.strip():
            st.error("‚ùå Could not extract text from this PDF.")
            st.stop()

        # -----------------------------
        # ‚úÇÔ∏è Chunk text
        # -----------------------------
        st.info("üîç Splitting text into chunks...")
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
        texts = splitter.split_text(all_text)
        st.write(f"üìö Created {len(texts)} chunks.")

        # -----------------------------
        # üíæ Create / load Chroma store
        # -----------------------------
        embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)

        # Create a new Chroma vector store from the text chunks
        st.session_state.vectordb = Chroma.from_texts(
            texts=texts,
            embedding=embeddings,
            persist_directory=unique_chroma_db_dir
        )
        st.success("Chunking and embedding completed successfully!")

    # Load existing Chroma store if the file is the same or if it was just created
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    if st.session_state.vectordb is None:
        st.session_state.vectordb = Chroma(
            persist_directory=unique_chroma_db_dir,
            embedding_function=embeddings
        )
    # Create a retriever for the vector store
    retriever = st.session_state.vectordb.as_retriever(search_kwargs={"k": 3})

    # -----------------------------
    # üß† Hugging Face Client (LLM)
    # -----------------------------
    HF_MODEL_ID = model_name
    client = InferenceClient(model=HF_MODEL_ID, token=HF_TOKEN)

    def call_llm(prompt):
        """
        Calls the Hugging Face language model to get a response.

        Args:
            prompt (str): The prompt to send to the language model.

        Returns:
            str: The response from the language model.
        """
        response = client.chat_completion(
            model=HF_MODEL_ID,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=512,
        )
        return response.choices[0].message["content"]

    # -----------------------------
    # üí¨ Ask a question
    # -----------------------------
    st.success("You can now ask your question:")
    query = st.text_input("Ask a question about your PDF:")

    if query:
        with st.spinner("ü§ñ Thinking..."):
            # Retrieve relevant documents from the vector store
            docs = retriever.invoke(query)
            context = "\n\n".join([d.page_content for d in docs])

            # Create a prompt for the language model
            prompt_template = """Answer the user's question truthfully and *only* based on the provided context. Do NOT use any external knowledge.
    If the answer is not found *within the provided context*, respond with "I cannot answer this question based on the provided document."

            Context:
            {context}

            Question:
            {question}"""

            prompt = prompt_template.format(context=context, question=query)
            # Get the answer from the language model
            answer = call_llm(prompt)

        st.subheader("üí° Answer")
        st.write(answer)

        with st.expander("üìò Retrieved Context"):
            st.write(context)

def cleanup_chroma_db():
    """
    Cleans up the Chroma database directory on application exit.
    
    This function is registered to be called at exit and will attempt to remove
    all subdirectories within the Chroma database directory. It includes retries
    with delays to handle potential PermissionError issues.
    """
    print(f"Attempting to clean up Chroma DB directory: {CHROMA_DB_DIR}")
    # Add a small delay to allow file handles to be released
    time.sleep(0.5)

    if "vectordb" in st.session_state and st.session_state.vectordb is not None:
        try:
            # Attempt to close the underlying Chroma client if it has a close method
            if hasattr(st.session_state.vectordb._client, 'close'):
                st.session_state.vectordb._client.close()
                print("  Chroma client explicitly closed.")
            st.session_state.vectordb = None
            gc.collect()
            time.sleep(0.5) # Additional delay after closing
        except Exception as e:
            print(f"  Error closing Chroma client: {e}")

    if os.path.exists(CHROMA_DB_DIR):
        for item in os.listdir(CHROMA_DB_DIR):
            item_path = os.path.join(CHROMA_DB_DIR, item)
            if os.path.isdir(item_path):
                print(f"  Deleting subdirectory: {item_path}")
                for i in range(5):
                    try:
                        shutil.rmtree(item_path)
                        print(f"    Successfully deleted: {item_path}")
                        break
                    except PermissionError as e:
                        print(f"    PermissionError deleting {item_path}: {e}. Retrying in 1 second...")
                        time.sleep(1)
                else:
                    print(f"    Failed to delete {item_path} after multiple retries due to PermissionError.")
    print("Chroma DB cleanup attempt finished.")

# Register the cleanup function to be called on application exit
atexit.register(cleanup_chroma_db)
